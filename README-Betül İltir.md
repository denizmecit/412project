I have developed a regression workflow on Instagram post/profile data with the goal of predicting a numeric engagement metric (such as popularity) on a transformed (log) scale. This solution integrates numerical and textual features: While Naive Bayes and Random Forest were explored for the regression models, their performance was limited compared to expectations. The project produces a test set that outputs predictions in JSON format, mapping post IDs to predicted engagement scores.
Data Description: The training data is in the file training-dataset.jsonl.gz, which contains multiple JSON records. Each record includes profile details (such as username, biography, category_name, entities, follower_count, and post_count) and a list of posts with like_count, comments_count, etc. Numeric features are aggregated (e.g., sums or averages), and text fields are prepared for embedding or vectorization. The test data, such as test-regression-round3.jsonl, includes posts with caption, comments_count, and ID fields. Placeholders are used for any missing features during processing.
Methodology: For data loading and preprocessing, JSON files are parsed into dictionaries and converted to Pandas DataFrames. Numeric features are aggregated, and log transformations are applied to handle skewed data. Text data is vectorized using TF-IDF, capturing term importance across records. The workflow included Naive Bayes for text-based predictions and Random Forest for handling structured data, but these classical models struggled to capture the complexity of the dataset. The target variable is log-transformed engagement metrics like log10(sum_like_count + 1). After splitting data into training and validation sets, models are trained, and hyperparameters are tuned for optimal results.
Evaluation metrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and RÂ² scores. Predictions are inverted from log space using 10^(prediction) - 1 for interpretability. For predictions, test data is processed through the same pipeline, and the results are saved in JSON format as { "post_id": predicted_engagement_score }.
The key takeaways highlight the limitations of Naive Bayes and Random Forest in handling complex, high-dimensional data, especially when textual and numerical features are combined. While log transformations were effective in managing skewed distributions, the classical models fell short in delivering optimal performance. Potential extensions of this project include adopting advanced machine learning techniques, integrating domain-specific preprocessing, hyperparameter tuning, and experimenting with models like XGBoost or deep learning approaches (e.g., BERT embeddings).
This project underscores the importance of aligning model complexity with dataset characteristics to achieve robust predictions while offering insights into the limitations of classical regression techniques in modern datasets.
